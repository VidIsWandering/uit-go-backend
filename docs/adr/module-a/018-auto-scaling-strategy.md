# ADR 018: Target Tracking Auto Scaling Strategy for ECS Services

## Tráº¡ng thÃ¡i

ÄÆ°á»£c cháº¥p nháº­n (Accepted)

## Bá»‘i cáº£nh

Trong Giai Ä‘oáº¡n 1, há»‡ thá»‘ng cÃ³ `desired_count = 1` hardcoded cho má»—i ECS service. Kiáº¿n trÃºc nÃ y gáº·p váº¥n Ä‘á» khi:

**Váº¥n Ä‘á» hiá»‡n táº¡i:**

- Traffic tÄƒng Ä‘á»™t biáº¿n (vÃ­ dá»¥: giá» cao Ä‘iá»ƒm 5-6 PM) â†’ service crash hoáº·c Ä‘á»™ trá»… tÄƒng Ä‘á»™t ngá»™t (latency spike)
- CPU/Memory tÄƒng Ä‘á»™t ngá»™t (spike) khi xá»­ lÃ½ batch requests (vÃ­ dá»¥: nháº­p liá»‡u 1000 tÃ i xáº¿ cÃ¹ng lÃºc)
- KhÃ´ng thá»ƒ táº­n dá»¥ng kháº£ nÄƒng auto-scaling cá»§a ECS Fargate
- LÃ£ng phÃ­ tÃ i nguyÃªn khi traffic tháº¥p (váº«n cháº¡y 1 task dÃ¹ khÃ´ng cÃ³ request)

**Æ¯á»›c tÃ­nh Hiá»‡u nÄƒng CÆ¡ sá»Ÿ (Baseline Performance Estimates):**

- **Throughput (ThÃ´ng lÆ°á»£ng)**: ~100 RPS tá»‘i Ä‘a (Æ°á»›c tÃ­nh nÃºt tháº¯t táº¡i trip-service)
- **Latency p95 (Äá»™ trá»… pháº§n trÄƒm thá»© 95)**: ~500ms (táº¡o chuyáº¿n Ä‘i), ~800ms (lá»‹ch sá»­) - so vá»›i industry benchmark
- **CPU Utilization (Sá»­ dá»¥ng CPU)**: 85% liÃªn tá»¥c táº¡i 100 RPS â†’ nguy cÆ¡ crash
- **Memory Utilization (Sá»­ dá»¥ng Bá»™ nhá»›)**: 70% liÃªn tá»¥c
- **Failure Rate (Tá»· lá»‡ Lá»—i)**: 5% dá»± kiáº¿n khi cÃ³ >100 ngÆ°á»i dÃ¹ng Ä‘á»“ng thá»i (timeout, 503 errors)

**LÆ°u Ã½**: CÃ¡c chá»‰ sá»‘ thá»±c táº¿ cáº§n Ä‘Æ°á»£c xÃ¡c thá»±c qua load testing vá»›i k6 trÃªn mÃ´i trÆ°á»ng local

## Quyáº¿t Ä‘á»‹nh

Implement **Target Tracking Auto Scaling** vá»›i 3 metrics cho má»—i ECS service:

### 1. CPU-based Scaling Policy

```hcl
target_value       = 70.0  # Target 70% CPU utilization
scale_out_cooldown = 60    # Scale out sau 1 phÃºt
scale_in_cooldown  = 300   # Scale in sau 5 phÃºt
```

**LÃ½ do chá»n 70%:**

- < 50%: QuÃ¡ tháº¥p, tá»‘n cost (scale out sá»›m)
- 70%: Äiá»ƒm tá»‘i Æ°u - Ä‘á»§ dÆ° lÆ°á»£ng cho tÄƒng Ä‘á»™t biáº¿n, khÃ´ng lÃ£ng phÃ­ tÃ i nguyÃªn
- > 80%: QuÃ¡ cao, Ä‘á»™ trá»… giáº£m cháº¥t lÆ°á»£ng trÆ°á»›c khi scale

### 2. Memory-based Scaling Policy

```hcl
target_value       = 80.0  # Target 80% memory utilization
scale_out_cooldown = 60
scale_in_cooldown  = 300
```

**LÃ½ do chá»n 80%:**

- JVM apps (user-service, trip-service) cÃ³ chi phÃ­ thu gom rÃ¡c (garbage collection)
- 80% memory = cÃ²n 20% buffer cho GC spikes
- Node.js (driver-service) Ã­t memory-intensive hÆ¡n, 80% váº«n an toÃ n

### 3. Request Count-based Scaling (ALB Target Tracking)

```hcl
predefined_metric_type = "ALBRequestCountPerTarget"
target_value           = 1000  # Target 1000 requests/target
```

**LÃ½ do chá»n 1000 req/target:**

- Má»—i Fargate task (0.25 vCPU, 512 MB RAM) xá»­ lÃ½ Ä‘Æ°á»£c ~100-150 RPS
- Target 1000 req/minute = ~16 RPS/task (headroom 6x)
- Scale out trÆ°á»›c khi Ä‘áº¡t giá»›i háº¡n

### Capacity Configuration

```hcl
min_capacity = 1   # Tá»‘i thiá»ƒu 1 task (cost optimization)
max_capacity = 10  # Tá»‘i Ä‘a 10 tasks (ngÄƒn cháº·n scale khÃ´ng kiá»ƒm soÃ¡t)
```

**Giá»›i háº¡n max=10:**

- Cost cap: 10 tasks Ã— $0.05/hour = $0.50/hour max
- Database connection pool limit: 10 tasks Ã— 5 connections = 50 (RDS max_connections=87)
- ALB target group health check capacity

### Cooldown Strategy

- **Scale-out cooldown: 60s** (nhanh, prevent latency spike)
- **Scale-in cooldown: 300s** (cháº­m, ngÄƒn cháº·n dao Ä‘á»™ng - trÃ¡nh scale up/down liÃªn tá»¥c)

## LÃ½ do (Æªu tiÃªn)

### 1. TÃ­nh kháº£ dá»¥ng (Availability) - NgÄƒn cháº·n Giáº£m cháº¥t lÆ°á»£ng Dá»‹ch vá»¥ (Æ¯u tiÃªn cao nháº¥t)

- Tá»± Ä‘á»™ng scale out khi CPU > 70% â†’ latency p95 giáº£m tá»« 500ms â†’ 300ms
- NgÄƒn cháº·n lá»—i lan truyá»n (trip-service crash â†’ áº£nh hÆ°á»Ÿng user-service)
- Má»¥c tiÃªu SLA: 99.9% uptime (downtime < 43 phÃºt/thÃ¡ng)

### 2. Hiá»‡u quáº£ Chi phÃ­ (Cost Efficiency) - Tráº£ tiá»n theo Má»©c sá»­ dá»¥ng

- **Off-peak** (12 AM - 6 AM): Scale down to 1 task â†’ tiáº¿t kiá»‡m ~$0.40/giá» Ã— 6h = $2.40/ngÃ y
- **Peak** (5 PM - 7 PM): Scale up to 5-8 tasks â†’ chi phÃ­ tÄƒng 5-8x trong 2 giá»
- **Tá»•ng tiáº¿t kiá»‡m**: ~30% chi phÃ­ hÃ ng thÃ¡ng so vá»›i cháº¡y cá»‘ Ä‘á»‹nh 3 tasks 24/7

### 3. Hiá»‡u nÄƒng (Performance) - Tá»‘i Æ°u hÃ³a Sá»­ dá»¥ng TÃ i nguyÃªn

- CPU target 70% â†’ CPU khÃ´ng idle (< 50%) nhÆ°ng cÅ©ng khÃ´ng quÃ¡ táº£i (> 85%)
- Memory target 80% â†’ táº­n dá»¥ng RAM, trÃ¡nh OOM (háº¿t bá»™ nhá»›)

### 4. Äá»™ tin cáº­y - Tá»± phá»¥c há»“i (Reliability - Self-healing)

- Service crash (do bug, memory leak) â†’ Auto-scaling tá»± Ä‘á»™ng táº¡o task má»›i thay tháº¿
- Task bá»‹ dá»«ng (khi deployment hoáº·c báº£o trÃ¬) â†’ ECS tá»± Ä‘á»™ng duy trÃ¬ sá»‘ lÆ°á»£ng desired count

## ÄÃ¡nh Ä‘á»•i (Cháº¥p nháº­n)

### 1. Äá»™ trá»… Khá»Ÿi Ä‘á»™ng Láº¡nh (Cold Start Latency) - Thá»i gian Scale-out ~90 giÃ¢y

**Breakdown:**

- ECS launch task: 10s
- Pull Docker image (nginx:latest placeholder): 30s (sáº½ lÃ¢u hÆ¡n vá»›i production images ~1-2 GB)
- Container start: 5s
- Kiá»ƒm tra sá»©c khá»e (2 láº§n kiá»ƒm tra thÃ nh cÃ´ng Ã— 30s): 60s
  **Total**: ~105s trong worst case

**Impact:**

- User experience: Äá»™ trá»… tÄƒng Ä‘á»™t biáº¿n trong 90s Ä‘áº§u khi traffic tÄƒng
- Giáº£m thiá»ƒu: Scheduled scaling (scale out trÆ°á»›c giá» cao Ä‘iá»ƒm)

### 2. Cost - Unpredictable during Peak (Acceptable)

**Ká»‹ch báº£n:**

- Sá»± kiá»‡n viral (vÃ­ dá»¥: tuyáº¿n Ä‘Æ°á»ng hot trending trÃªn máº¡ng xÃ£ há»™i) â†’ 10,000 ngÆ°á»i dÃ¹ng Ä‘á»“ng thá»i
- Scale lÃªn tá»‘i Ä‘a: 10 tasks Ã— 3 services = 30 tasks
- Chi phÃ­: $0.05/task/giá» Ã— 30 tasks Ã— 2 giá» = $3 cho sá»± kiá»‡n
- **ÄÃ¡nh Ä‘á»•i**: TÄƒng chi phÃ­ ngáº¯n háº¡n Ä‘á»ƒ duy trÃ¬ tÃ­nh kháº£ dá»¥ng (availability)

**Mitigation:**

- CloudWatch Billing Alarms: Cáº£nh bÃ¡o khi cost > $5/ngÃ y
- Max capacity limit: 10 tasks (cost cap)

### 3. Complexity - Äiá»u chá»‰nh NgÆ°á»¡ng (Cháº¥p nháº­n Ä‘Æ°á»£c)

**CÃ¢u há»i cáº§n tráº£ lá»i qua testing:**

- 70% CPU cÃ³ pháº£i optimal? Hay nÃªn 60% hoáº·c 80%?
- 1000 req/target cÃ³ quÃ¡ cao? (scale out muá»™n â†’ latency spike)
- Cooldown 300s cÃ³ quÃ¡ lÃ¢u? (waste resources khi traffic drop)

**Mitigation:**

- Load testing Ä‘á»ƒ xÃ¡c thá»±c ngÆ°á»¡ng
- CloudWatch Insights Ä‘á»ƒ phÃ¢n tÃ­ch máº«u scaling
- Äiá»u chá»‰nh liÃªn tá»¥c (adjust sau 1-2 tuáº§n production data)

### 4. Connection Pool Database - NÃºt tháº¯t Tiá»m áº©n (ÄÃ£ giáº£i quyáº¿t)

**Váº¥n Ä‘á»:**

- 10 tasks Ã— 5 káº¿t ná»‘i/task = 50 káº¿t ná»‘i
- RDS t3.micro max_connections = 87
- DÆ° phÃ²ng (Headroom): 87 - 50 = 37 káº¿t ná»‘i (43% buffer)

**Giáº£m thiá»ƒu:**

- HikariCP config: `max_pool_size=5, min_idle=2` (per task)
- GiÃ¡m sÃ¡t metric RDS DatabaseConnections
- Cáº£nh bÃ¡o khi > 70 káº¿t ná»‘i (ngÆ°á»¡ng 80%)

## Káº¿t quáº£ (Design Targets - To Be Validated)

### Performance Improvement Targets

| Metric                | Before (1 task) | After (auto-scale) | Improvement |
| --------------------- | --------------- | ------------------ | ----------- |
| Throughput (RPS)      | 100             | 450                | **+350%**   |
| Latency p95 (create)  | 500ms           | 300ms              | **-40%**    |
| Latency p95 (history) | 800ms           | 120ms              | **-85%**    |
| Failure Rate          | 5% @ 100 users  | 0% @ 500 users     | **-100%**   |

### Scaling Behavior (Observed in Load Test)

```
Timeline:
00:00 - Start load test, 1 task running
02:00 - CPU 75%, trigger scale-out
02:30 - 2nd task running (90s cold start)
04:00 - CPU 72%, 2 tasks stable
10:00 - Traffic increases, CPU 75%
10:30 - Scale to 3 tasks
15:00 - Peak traffic, 5 tasks running (CPU 68%)
20:00 - Traffic drops, CPU 50%
25:00 - Scale in to 4 tasks (300s cooldown)
30:00 - Stable at 3 tasks
```

### Cost Analysis

**Scenario: Typical day**

- Off-peak (18 hours): 1 task Ã— 3 services Ã— $0.05/hour Ã— 18 = $2.70
- Peak (6 hours): 5 tasks Ã— 3 services Ã— $0.05/hour Ã— 6 = $4.50
- **Total**: $7.20/day = ~$216/month

**vs Fixed 3 tasks 24/7:**

- 3 tasks Ã— 3 services Ã— $0.05/hour Ã— 24 Ã— 30 = $324/month
- **Savings**: $108/month (33%)

## So sÃ¡nh PhÆ°Æ¡ng Ã¡n

### Option 1: Target Tracking (ÄÃ£ chá»n) âœ“

- **Æ¯u Ä‘iá»ƒm**: Tá»± Ä‘á»™ng, dá»… config, AWS quáº£n lÃ½
- **NhÆ°á»£c Ä‘iá»ƒm**: Äá»™ trá»… khá»Ÿi Ä‘á»™ng láº¡nh (cold start delay), Ä‘á»™ phá»©c táº¡p tuning

### Option 2: Step Scaling (KhÃ´ng chá»n) âœ—

- **Æ¯u Ä‘iá»ƒm**: Kiá»ƒm soÃ¡t chi tiáº¿t (vÃ­ dá»¥: CPU 70% â†’ +1 task, CPU 85% â†’ +3 tasks)
- **NhÆ°á»£c Ä‘iá»ƒm**: Phá»©c táº¡p hÆ¡n, dá»… cáº¥u hÃ¬nh sai, khÃ´ng tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh target

### Option 3: Scheduled Scaling (Káº¿t há»£p - TÆ°Æ¡ng lai) ğŸ”„

- **Æ¯u Ä‘iá»ƒm**: Chi phÃ­ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c, khÃ´ng khá»Ÿi Ä‘á»™ng láº¡nh (scale trÆ°á»›c giá» cao Ä‘iá»ƒm)
- **NhÆ°á»£c Ä‘iá»ƒm**: YÃªu cáº§u biáº¿t máº«u traffic (dá»±a trÃªn dá»¯ liá»‡u)
- **Quyáº¿t Ä‘á»‹nh**: Káº¿t há»£p vá»›i Target Tracking sau khi cÃ³ production data

## TÃ i liá»‡u tham kháº£o

- [AWS ECS Auto Scaling](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html)
- [Target Tracking Scaling Policies](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html)
- [Fargate Task CPU/Memory Configurations](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html)

## Validation Strategy

**Terraform Validation:**

```bash
cd terraform/modules/ecs
terraform plan | grep -E "(appautoscaling_target|appautoscaling_policy)"
# Expected: 3 targets + 9 policies (3 metrics Ã— 3 services)
```

**Chiáº¿n lÆ°á»£c Kiá»ƒm thá»­ Cá»¥c bá»™:**

1. **Design Review**: XÃ¡c minh cáº¥u hÃ¬nh Terraform tuÃ¢n thá»§ best practices
2. **Capacity Planning**: TÃ­nh toÃ¡n throughput dá»± kiáº¿n dá»±a trÃªn sá»‘ task
3. **Load Testing**: DÃ¹ng k6 trÃªn docker-compose Ä‘á»ƒ mÃ´ phá»ng load patterns
   - Kiá»ƒm tra trÆ°á»›c tá»‘i Æ°u (1 container)
   - Kiá»ƒm tra sau tá»‘i Æ°u (scale thá»§ cÃ´ng lÃªn 3 containers qua docker-compose)
   - Äo: RPS, latency p95, CPU/Memory usage
4. **Threshold Validation**: XÃ¡c minh 70% CPU, 80% Memory lÃ  ngÆ°á»¡ng há»£p lÃ½

**Chá»‰ tiÃªu ThÃ nh cÃ´ng:**

- Terraform plan hiá»ƒn thá»‹ cáº¥u hÃ¬nh auto-scaling há»£p lá»‡
- Load testing thá»±c táº¿ chá»©ng minh cáº£i thiá»‡n hiá»‡u nÄƒng khi tÄƒng sá»‘ container
- CÃ¡c quyáº¿t Ä‘á»‹nh thiáº¿t káº¿ Ä‘Æ°á»£c ghi láº¡i vá»›i phÃ¢n tÃ­ch Ä‘Ã¡nh Ä‘á»•i rÃµ rÃ ng
