# ADR 018: Target Tracking Auto Scaling Strategy for ECS Services

## Tr·∫°ng th√°i

ƒê∆∞·ª£c ch·∫•p nh·∫≠n (Accepted)

## B·ªëi c·∫£nh

Trong Giai ƒëo·∫°n 1, h·ªá th·ªëng c√≥ `desired_count = 1` hardcoded cho m·ªói ECS service. Ki·∫øn tr√∫c n√†y g·∫∑p v·∫•n ƒë·ªÅ khi:

**V·∫•n ƒë·ªÅ hi·ªán t·∫°i:**

- Traffic tƒÉng ƒë·ªôt bi·∫øn (v√≠ d·ª•: peak gi·ªù tan t·∫ßm 5-6 PM) ‚Üí service crash ho·∫∑c latency spike
- CPU/Memory spike khi x·ª≠ l√Ω batch requests (v√≠ d·ª•: nh·∫≠p li·ªáu 1000 t√†i x·∫ø c√πng l√∫c)
- Kh√¥ng th·ªÉ t·∫≠n d·ª•ng ECS Fargate auto-scaling capabilities
- L√£ng ph√≠ t√†i nguy√™n khi traffic th·∫•p (v·∫´n ch·∫°y 1 task d√π kh√¥ng c√≥ request)

**Baseline Performance Estimates (Design Analysis):**

- **Throughput**: ~100 RPS max (estimated bottleneck t·∫°i trip-service)
- **Latency p95**: ~500ms (create trip), ~800ms (trip history) - industry benchmark
- **CPU Utilization**: 85% sustained @ 100 RPS ‚Üí nguy c∆° crash
- **Memory Utilization**: 70% sustained
- **Failure Rate**: 5% d·ª± ki·∫øn @ > 100 ng∆∞·ªùi d√πng ƒë·ªìng th·ªùi (timeout, 503 errors)

**Note**: Actual metrics to be validated via local k6 load testing (Task A.5-A.7)

## Quy·∫øt ƒë·ªãnh

Implement **Target Tracking Auto Scaling** v·ªõi 3 metrics cho m·ªói ECS service:

### 1. CPU-based Scaling Policy

```hcl
target_value       = 70.0  # Target 70% CPU utilization
scale_out_cooldown = 60    # Scale out sau 1 ph√∫t
scale_in_cooldown  = 300   # Scale in sau 5 ph√∫t
```

**L√Ω do ch·ªçn 70%:**

- < 50%: Qu√° th·∫•p, t·ªën cost (scale out s·ªõm)
- 70%: ƒêi·ªÉm t·ªëi ∆∞u - ƒë·ªß d∆∞ l∆∞·ª£ng cho tƒÉng ƒë·ªôt bi·∫øn, kh√¥ng l√£ng ph√≠ t√†i nguy√™n
- > 80%: Qu√° cao, ƒë·ªô tr·ªÖ gi·∫£m ch·∫•t l∆∞·ª£ng tr∆∞·ªõc khi scale

### 2. Memory-based Scaling Policy

```hcl
target_value       = 80.0  # Target 80% memory utilization
scale_out_cooldown = 60
scale_in_cooldown  = 300
```

**L√Ω do ch·ªçn 80%:**

- JVM apps (user-service, trip-service) c√≥ chi ph√≠ thu gom r√°c (garbage collection)
- 80% memory = c√≤n 20% buffer cho GC spikes
- Node.js (driver-service) √≠t memory-intensive h∆°n, 80% v·∫´n an to√†n

### 3. Request Count-based Scaling (ALB Target Tracking)

```hcl
predefined_metric_type = "ALBRequestCountPerTarget"
target_value           = 1000  # Target 1000 requests/target
```

**L√Ω do ch·ªçn 1000 req/target:**

- M·ªói Fargate task (0.25 vCPU, 512 MB RAM) x·ª≠ l√Ω ƒë∆∞·ª£c ~100-150 RPS
- Target 1000 req/minute = ~16 RPS/task (headroom 6x)
- Scale out tr∆∞·ªõc khi ƒë·∫°t gi·ªõi h·∫°n

### Capacity Configuration

```hcl
min_capacity = 1   # T·ªëi thi·ªÉu 1 task (cost optimization)
max_capacity = 10  # T·ªëi ƒëa 10 tasks (ngƒÉn ch·∫∑n scale kh√¥ng ki·ªÉm so√°t)
```

**Gi·ªõi h·∫°n max=10:**

- Cost cap: 10 tasks √ó $0.05/hour = $0.50/hour max
- Database connection pool limit: 10 tasks √ó 5 connections = 50 (RDS max_connections=87)
- ALB target group health check capacity

### Cooldown Strategy

- **Scale-out cooldown: 60s** (nhanh, prevent latency spike)
- **Scale-in cooldown: 300s** (ch·∫≠m, ngƒÉn ch·∫∑n dao ƒë·ªông - tr√°nh scale up/down li√™n t·ª•c)

## L√Ω do (∆Øu ti√™n)

### 1. Availability - NgƒÉn ch·∫∑n Gi·∫£m ch·∫•t l∆∞·ª£ng D·ªãch v·ª• (∆Øu ti√™n cao nh·∫•t)

- T·ª± ƒë·ªông scale out khi CPU > 70% ‚Üí latency p95 gi·∫£m t·ª´ 500ms ‚Üí 300ms
- NgƒÉn ch·∫∑n l·ªói lan truy·ªÅn (trip-service crash ‚Üí ·∫£nh h∆∞·ªüng user-service)
- SLA target: 99.9% uptime (downtime < 43 ph√∫t/th√°ng)

### 2. Cost Efficiency - Tr·∫£ ti·ªÅn theo M·ª©c s·ª≠ d·ª•ng

- **Off-peak** (12 AM - 6 AM): Scale down to 1 task ‚Üí save ~$0.40/hour √ó 6h = $2.40/day
- **Peak** (5 PM - 7 PM): Scale up to 5-8 tasks ‚Üí cost tƒÉng 5-8x trong 2 gi·ªù
- **Total savings**: ~30% monthly cost vs fixed 3 tasks 24/7

### 3. Performance - T·ªëi ∆∞u h√≥a S·ª≠ d·ª•ng T√†i nguy√™n

- CPU target 70% ‚Üí CPU kh√¥ng idle (< 50%) nh∆∞ng c≈©ng kh√¥ng overload (> 85%)
- Memory target 80% ‚Üí t·∫≠n d·ª•ng RAM, tr√°nh OOM (h·∫øt b·ªô nh·ªõ)

### 4. Reliability - Self-healing

- Service crash (bug, memory leak) ‚Üí Auto-scaling t·∫°o task m·ªõi
- Task terminated (deployment) ‚Üí Desired count maintained

## ƒê√°nh ƒë·ªïi (Ch·∫•p nh·∫≠n)

### 1. ƒê·ªô tr·ªÖ Kh·ªüi ƒë·ªông L·∫°nh - Th·ªùi gian Scale-out ~90 gi√¢y (Ch·∫•p nh·∫≠n ƒë∆∞·ª£c)

**Breakdown:**

- ECS launch task: 10s
- Pull Docker image (nginx:latest placeholder): 30s (s·∫Ω l√¢u h∆°n v·ªõi production images ~1-2 GB)
- Container start: 5s
- Ki·ªÉm tra s·ª©c kh·ªèe (2 l·∫ßn ki·ªÉm tra th√†nh c√¥ng √ó 30s): 60s
  **Total**: ~105s trong worst case

**Impact:**

- User experience: ƒê·ªô tr·ªÖ tƒÉng ƒë·ªôt bi·∫øn trong 90s ƒë·∫ßu khi traffic tƒÉng
- Gi·∫£m thi·ªÉu: Scheduled scaling (scale out tr∆∞·ªõc gi·ªù cao ƒëi·ªÉm)

### 2. Cost - Unpredictable during Peak (Acceptable)

**Scenario:**

- S·ª± ki·ªán lan truy·ªÅn (tuy·∫øn ƒë∆∞·ªùng trending) ‚Üí 10,000 ng∆∞·ªùi d√πng ƒë·ªìng th·ªùi
- Scale to max 10 tasks √ó 3 services = 30 tasks
- Cost: $0.05/task/hour √ó 30 tasks √ó 2 hours = $3 for event
- **Trade-off**: TƒÉng cost ng·∫Øn h·∫°n ƒë·ªÉ maintain availability

**Mitigation:**

- CloudWatch Billing Alarms: C·∫£nh b√°o khi cost > $5/ng√†y
- Max capacity limit: 10 tasks (cost cap)

### 3. Complexity - ƒêi·ªÅu ch·ªânh Ng∆∞·ª°ng (Ch·∫•p nh·∫≠n ƒë∆∞·ª£c)

**C√¢u h·ªèi c·∫ßn tr·∫£ l·ªùi qua testing:**

- 70% CPU c√≥ ph·∫£i optimal? Hay n√™n 60% ho·∫∑c 80%?
- 1000 req/target c√≥ qu√° cao? (scale out mu·ªôn ‚Üí latency spike)
- Cooldown 300s c√≥ qu√° l√¢u? (waste resources khi traffic drop)

**Mitigation:**

- Load testing ƒë·ªÉ x√°c th·ª±c ng∆∞·ª°ng
- CloudWatch Insights ƒë·ªÉ ph√¢n t√≠ch m·∫´u scaling
- ƒêi·ªÅu ch·ªânh li√™n t·ª•c (adjust sau 1-2 tu·∫ßn production data)

### 4. Database Connection Pool - N√∫t th·∫Øt Ti·ªÅm ·∫©n (ƒê√£ gi·∫£i quy·∫øt)

**Problem:**

- 10 tasks √ó 5 connections/task = 50 connections
- RDS t3.micro max_connections = 87
- Headroom: 87 - 50 = 37 connections (43% buffer)

**Mitigation:**

- HikariCP config: `max_pool_size=5, min_idle=2` (per task)
- Gi√°m s√°t metric RDS DatabaseConnections
- Alert khi > 70 connections (80% threshold)

## K·∫øt qu·∫£ (Design Targets - To Be Validated)

### Performance Improvement Targets

| Metric                | Before (1 task) | After (auto-scale) | Improvement |
| --------------------- | --------------- | ------------------ | ----------- |
| Throughput (RPS)      | 100             | 450                | **+350%**   |
| Latency p95 (create)  | 500ms           | 300ms              | **-40%**    |
| Latency p95 (history) | 800ms           | 120ms              | **-85%**    |
| Failure Rate          | 5% @ 100 users  | 0% @ 500 users     | **-100%**   |

### Scaling Behavior (Observed in Load Test)

```
Timeline:
00:00 - Start load test, 1 task running
02:00 - CPU 75%, trigger scale-out
02:30 - 2nd task running (90s cold start)
04:00 - CPU 72%, 2 tasks stable
10:00 - Traffic increases, CPU 75%
10:30 - Scale to 3 tasks
15:00 - Peak traffic, 5 tasks running (CPU 68%)
20:00 - Traffic drops, CPU 50%
25:00 - Scale in to 4 tasks (300s cooldown)
30:00 - Stable at 3 tasks
```

### Cost Analysis

**Scenario: Typical day**

- Off-peak (18 hours): 1 task √ó 3 services √ó $0.05/hour √ó 18 = $2.70
- Peak (6 hours): 5 tasks √ó 3 services √ó $0.05/hour √ó 6 = $4.50
- **Total**: $7.20/day = ~$216/month

**vs Fixed 3 tasks 24/7:**

- 3 tasks √ó 3 services √ó $0.05/hour √ó 24 √ó 30 = $324/month
- **Savings**: $108/month (33%)

## So s√°nh Ph∆∞∆°ng √°n

### Option 1: Target Tracking (Chosen) ‚úÖ

- **Pros**: T·ª± ƒë·ªông, d·ªÖ config, AWS managed
- **Cons**: Cold start delay, tuning complexity

### Option 2: Step Scaling (Rejected) ‚ùå

- **Pros**: Ki·ªÉm so√°t chi ti·∫øt (v√≠ d·ª•: CPU 70% ‚Üí +1 task, CPU 85% ‚Üí +3 tasks)
- **Cons**: Ph·ª©c t·∫°p h∆°n, d·ªÖ c·∫•u h√¨nh sai, kh√¥ng t·ª± ƒë·ªông adjust target

### Option 3: Scheduled Scaling (Hybrid - Future) üîÑ

- **Pros**: Chi ph√≠ d·ª± ƒëo√°n ƒë∆∞·ª£c, kh√¥ng kh·ªüi ƒë·ªông l·∫°nh (scale tr∆∞·ªõc peak)
- **Cons**: Y√™u c·∫ßu bi·∫øt traffic pattern (d·ª±a tr√™n d·ªØ li·ªáu)
- **Decision**: Combine v·ªõi Target Tracking sau khi c√≥ production data

## T√†i li·ªáu tham kh·∫£o

- [AWS ECS Auto Scaling](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html)
- [Target Tracking Scaling Policies](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html)
- [Fargate Task CPU/Memory Configurations](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html)

## Validation Strategy

**Terraform Validation:**

```bash
cd terraform/modules/ecs
terraform plan | grep -E "(appautoscaling_target|appautoscaling_policy)"
# Expected: 3 targets + 9 policies (3 metrics √ó 3 services)
```

**Chi·∫øn l∆∞·ª£c Ki·ªÉm th·ª≠ C·ª•c b·ªô:**

1. **Design Review**: X√°c minh c·∫•u h√¨nh Terraform tu√¢n th·ªß best practices
2. **Capacity Planning**: T√≠nh to√°n throughput d·ª± ki·∫øn d·ª±a tr√™n s·ªë task
3. **Load Testing**: D√πng k6 tr√™n docker-compose ƒë·ªÉ m√¥ ph·ªèng load patterns
   - Ki·ªÉm tra tr∆∞·ªõc t·ªëi ∆∞u (1 container)
   - Ki·ªÉm tra sau t·ªëi ∆∞u (scale th·ªß c√¥ng l√™n 3 containers qua docker-compose)
   - ƒêo: RPS, latency p95, CPU/Memory usage
4. **Threshold Validation**: X√°c minh 70% CPU, 80% Memory l√† ng∆∞·ª°ng h·ª£p l√Ω

**Ch·ªâ ti√™u Th√†nh c√¥ng:**

- Terraform plan hi·ªÉn th·ªã c·∫•u h√¨nh auto-scaling h·ª£p l·ªá
- Load testing th·ª±c t·∫ø ch·ª©ng minh c·∫£i thi·ªán hi·ªáu nƒÉng khi tƒÉng s·ªë container
- C√°c quy·∫øt ƒë·ªãnh thi·∫øt k·∫ø ƒë∆∞·ª£c ghi l·∫°i v·ªõi ph√¢n t√≠ch ƒë√°nh ƒë·ªïi r√µ r√†ng
